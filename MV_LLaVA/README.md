# MV-LLaVA

## üìú News
MV LLaVA is trained on 30K GPT-4V generated instructive conversation pairs, enable LLaVA to process multi-view images rendered from 3D content, chat about it and generate dense descriptive captions or provide quality estimation.

It's 7B model is available on [huggingface](https://huggingface.co/Zery/MV-LLaVA-7B)

We use this model to provide quality estimation on Objaverse and rewrite dense descriptive captions, We call this caption dataset BS-Objaverse(**B**oot**S**trap-Objaverse), it is now available on [huggingface](https://huggingface.co/datasets/Zery/BS-Objaverse).

We also use this model to process synthetic multi-view images generated by SV3D and Zero123++, It's full version of 1M high quality multi-view images will be made publicly available soon.


## üõ†Ô∏è Usage
### Installation
Our MV-LLaVA is based on ShareGPT-4V, thanks for their awesome work!
You can clone our repo and `cd MV_LLaVA && pip install -e .` to install `share4v` package.

- launch our demo through `python app.py`
- batch inference your multi-view images using batch scripts in `tools/`
